{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lj6DO31nrreb",
        "3U6wFvCWEGun",
        "I-DedsknrpJn"
      ],
      "authorship_tag": "ABX9TyPLPgngC0aCOkER8ZvLRThR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "7ovABqndutsN",
        "outputId": "277c64c6-ecb6-4ab2-cf9b-7b44ca95b74b"
      },
      "source": [
        "!pip install q keras==2.2.4\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: q in /usr/local/lib/python3.7/dist-packages (2.6)\n",
            "Collecting keras==2.2.4\n",
            "  Using cached https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.7 (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "Successfully installed keras-2.2.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ON7kzGQvlnN",
        "outputId": "a92aa0ff-8d35-4364-b220-e3ed1ed1d2a9"
      },
      "source": [
        "%tensorflow_version 1.15.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.15.5`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVAsV4OXvcQE",
        "outputId": "b7623145-186e-445f-f0b8-c54f16494aad"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "print(\"Tensorflow version %s\" %tf.__version__)\r\n",
        "print(\"Keras version %s\" %keras.__version__)\r\n",
        "\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "if device_name != '/device:GPU:0':\r\n",
        "  raise SystemError('GPU device not found')\r\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.4.1\n",
            "Keras version 2.4.3\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHkHclRnvhQI",
        "outputId": "c2b63c17-683e-4b97-ae6a-7a8e8d9052b8"
      },
      "source": [
        "import os\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "!pip3 install pickle5\r\n",
        "import pickle5 as pickle\r\n",
        "import pandas as pd\r\n",
        "with open('/content/drive/MyDrive/processed_data_frame_new.pkl', \"rb\") as fh:\r\n",
        "  data = pickle.load(fh)\r\n",
        "dataframe = pd.DataFrame(data)\r\n",
        "#read_pickle('/content/drive/MyDrive/processed_data_frame.pkl')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting pickle5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/5c4dd0462c8d3a6bc4af500a6af240763c2ebd1efdc736fc2c946d44b70a/pickle5-0.0.11.tar.gz (132kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 22.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=219274 sha256=71c8eb0e25268635f8c59adb7f2178a30ead17e0658b0e2ac3bf7b41c05c7a4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/90/95/f889ca4aa8b0e0c7f21c8470b6f5d6032f0390a3a141a9a3bd\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e4ut1Y6zdXs"
      },
      "source": [
        "dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHv0-kJ1vhYv"
      },
      "source": [
        "#from sklearn.preprocessing import LabelEncoder\r\n",
        "#from keras.utils import to_categorical\r\n",
        "\r\n",
        "# Convert features and corresponding classification labels into numpy arrays\r\n",
        "X = np.array(dataframe.feature.tolist())\r\n",
        "y = np.array(dataframe.class_label.tolist())\r\n",
        "\r\n",
        "# no need to encode labels since they are already in form of one hot encode\r\n",
        "\r\n",
        "# delete dataframe to save memory\r\n",
        "#reset_selective -f dataframe\r\n",
        "\r\n",
        "# split the dataset \r\n",
        "from sklearn.model_selection import train_test_split \r\n",
        "\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\r\n",
        "\r\n",
        "# delete X to save memory\r\n",
        "#reset_selective -f X"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj6DO31nrreb"
      },
      "source": [
        "## baseline model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u65NKa2RroZV",
        "outputId": "39fcb4a8-c43a-45b2-bcd3-00c0a68ab773"
      },
      "source": [
        "from keras.models import Model\r\n",
        "from keras.layers import SeparableConv2D, ZeroPadding2D, Activation, Dropout, Dense, \\\r\n",
        "                            Conv2D, MaxPooling2D, Reshape, GRU\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras import Input, optimizers\r\n",
        "\r\n",
        "num_rows = X.shape[1]\r\n",
        "num_columns = X.shape[2]\r\n",
        "num_channels = 1\r\n",
        "\r\n",
        "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\r\n",
        "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\r\n",
        "\r\n",
        "num_labels = y.shape[2]\r\n",
        "#num_labels = 16\r\n",
        "\r\n",
        "def Net(dilated_kernel, dilation, dilated_padding):\r\n",
        "    \r\n",
        "    # input layer\r\n",
        "    inputs = Input(shape=(num_rows, num_columns, num_channels))\r\n",
        "    \r\n",
        "    # DWS-CNN layer 1\r\n",
        "    x = ZeroPadding2D(padding=(2))(inputs)\r\n",
        "    # use valid padding since padding is introduced before due to its special form (maybe it's equal to use same padding?)\r\n",
        "    x = Conv2D(256,kernel_size=(5,5), strides=(1,1), padding='valid')(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "    # Batch Normalisation before passing it to the next layer\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    # Pooling\r\n",
        "    x = MaxPooling2D(pool_size=(1,5), strides=(1,5), padding='valid')(x)\r\n",
        "    # Dropout\r\n",
        "    x = Dropout(0.25)(x)\r\n",
        "    \r\n",
        "    # DWS-CNN layer 2\r\n",
        "    x = ZeroPadding2D(padding=(2))(x)\r\n",
        "    # use valid padding since padding is introduced before due to its special form\r\n",
        "    x = Conv2D(256,kernel_size=(5,5), strides=(1,1), padding='valid')(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "    # Batch Normalisation before passing it to the next layer\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    # Pooling\r\n",
        "    x = MaxPooling2D(pool_size=(1,4), strides=(1,4), padding='valid')(x)\r\n",
        "    # Dropout\r\n",
        "    x = Dropout(0.25)(x)\r\n",
        "    \r\n",
        "    # DWS-CNN layer 3\r\n",
        "    x = ZeroPadding2D(padding=(2))(x)\r\n",
        "    # use valid padding since padding is introduced before due to its special form\r\n",
        "    x = Conv2D(256,kernel_size=(5,5), strides=(1,1), padding='valid')(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "    # Batch Normalisation before passing it to the next layer\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    # Pooling\r\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2), padding='valid')(x)\r\n",
        "    # Dropout\r\n",
        "    x = Dropout(0.25)(x)\r\n",
        "    \r\n",
        "    x = Reshape((1024, 256))(x)\r\n",
        "    \r\n",
        "    # GRU\r\n",
        "    #x = ZeroPadding2D(padding=(dilated_padding*dilation, 1))(x)\r\n",
        "    #x = Conv2D(256, kernel_size=dilated_kernel, dilation_rate=(dilation, 1))(x)\r\n",
        "    #x = BatchNormalization()(x)\r\n",
        "    #x = Activation('relu')(x)\r\n",
        "    x = GRU(256, return_sequences=True)(x)\r\n",
        "        \r\n",
        "    # classifier layer\r\n",
        "    outputs = Dense(num_labels,activation='sigmoid')(x)\r\n",
        "    \r\n",
        "    \r\n",
        "    # model compilation for training\r\n",
        "    adam = optimizers.Adam(lr=0.0001)\r\n",
        "    model = Model(inputs, outputs)                            \r\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[\"binary_accuracy\"])\r\n",
        "    \r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "# create the model\r\n",
        "dilated_kernel = (3,3)\r\n",
        "dilation = 10\r\n",
        "dilated_padding = 1\r\n",
        "model = Net(dilated_kernel,dilation,dilated_padding)\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1024, 40, 1)]     0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2 (None, 1028, 44, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 1024, 40, 256)     6656      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 1024, 40, 256)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1024, 40, 256)     1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 1024, 8, 256)      0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024, 8, 256)      0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPaddin (None, 1028, 12, 256)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 1024, 8, 256)      1638656   \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1024, 8, 256)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1024, 8, 256)      1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1024, 2, 256)      0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024, 2, 256)      0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPaddin (None, 1028, 6, 256)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1024, 2, 256)      1638656   \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1024, 2, 256)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1024, 2, 256)      1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1024, 1, 256)      0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024, 1, 256)      0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 1024, 256)         0         \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 1024, 256)         394752    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024, 16)          4112      \n",
            "=================================================================\n",
            "Total params: 3,685,904\n",
            "Trainable params: 3,684,368\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U6wFvCWEGun"
      },
      "source": [
        "## Dessed model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un_24V80EGal",
        "outputId": "fb03362e-fb59-47c1-ccb1-2f78b3bf9b44"
      },
      "source": [
        "from keras.models import Model\r\n",
        "from keras.layers import SeparableConv2D, ZeroPadding2D, Activation, Dropout, Dense, \\\r\n",
        "                            Conv2D, MaxPooling2D, Reshape, GRU\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras import Input, optimizers\r\n",
        "\r\n",
        "num_rows = X.shape[1]\r\n",
        "num_columns = X.shape[2]\r\n",
        "num_channels = 1\r\n",
        "\r\n",
        "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\r\n",
        "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\r\n",
        "\r\n",
        "num_labels = y.shape[2]\r\n",
        "#num_labels = 16\r\n",
        "\r\n",
        "def Net():\r\n",
        "    \r\n",
        "    # input layer\r\n",
        "    inputs = Input(shape=(num_rows, num_columns, num_channels))\r\n",
        "    \r\n",
        "    # DWS-CNN layer 1\r\n",
        "    x = ZeroPadding2D(padding=(2))(inputs)\r\n",
        "    # use valid padding since padding is introduced before due to its special form (maybe it's equal to use same padding?)\r\n",
        "    x = SeparableConv2D(256,kernel_size=(5,5), strides=(1,1), padding='valid')(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "    # Batch Normalisation before passing it to the next layer\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    # Pooling\r\n",
        "    x = MaxPooling2D(pool_size=(1,5), strides=(1,5), padding='valid')(x)\r\n",
        "    # Dropout\r\n",
        "    x = Dropout(0.25)(x)\r\n",
        "    \r\n",
        "    # DWS-CNN layer 2\r\n",
        "    x = ZeroPadding2D(padding=(2))(x)\r\n",
        "    # use valid padding since padding is introduced before due to its special form\r\n",
        "    x = SeparableConv2D(256,kernel_size=(5,5), strides=(1,1), padding='valid')(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "    # Batch Normalisation before passing it to the next layer\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    # Pooling\r\n",
        "    x = MaxPooling2D(pool_size=(1,4), strides=(1,4), padding='valid')(x)\r\n",
        "    # Dropout\r\n",
        "    x = Dropout(0.25)(x)\r\n",
        "    \r\n",
        "    # DWS-CNN layer 3\r\n",
        "    x = ZeroPadding2D(padding=(2))(x)\r\n",
        "    # use valid padding since padding is introduced before due to its special form\r\n",
        "    x = SeparableConv2D(256,kernel_size=(5,5), strides=(1,1), padding='valid')(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "    # Batch Normalisation before passing it to the next layer\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    # Pooling\r\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2), padding='valid')(x)\r\n",
        "    # Dropout\r\n",
        "    x = Dropout(0.25)(x)\r\n",
        "    \r\n",
        "    x = Reshape((1024, 256))(x)\r\n",
        "    \r\n",
        "    # GRU\r\n",
        "    x = GRU(256, return_sequences=True)(x)\r\n",
        "        \r\n",
        "    # classifier layer\r\n",
        "    outputs = Dense(num_labels,activation='sigmoid')(x)\r\n",
        "    \r\n",
        "    \r\n",
        "    # model compilation for training\r\n",
        "    adam = optimizers.Adam(lr=0.0001)\r\n",
        "    model = Model(inputs, outputs)                            \r\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[\"binary_accuracy\"])\r\n",
        "    \r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "# create the model\r\n",
        "dilated_kernel = (3,3)\r\n",
        "dilation = 10\r\n",
        "dilated_padding = 1\r\n",
        "model = Net()\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1024, 40, 1)]     0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2 (None, 1028, 44, 1)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 1024, 40, 256)     537       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 1024, 40, 256)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1024, 40, 256)     1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 1024, 8, 256)      0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024, 8, 256)      0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPaddin (None, 1028, 12, 256)     0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_1 (Separabl (None, 1024, 8, 256)      72192     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1024, 8, 256)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1024, 8, 256)      1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1024, 2, 256)      0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024, 2, 256)      0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPaddin (None, 1028, 6, 256)      0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_2 (Separabl (None, 1024, 2, 256)      72192     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1024, 2, 256)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1024, 2, 256)      1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1024, 1, 256)      0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024, 1, 256)      0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 1024, 256)         0         \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 1024, 256)         394752    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024, 16)          4112      \n",
            "=================================================================\n",
            "Total params: 546,857\n",
            "Trainable params: 545,321\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr-7W-6GvaYp"
      },
      "source": [
        "## Baseline dilated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sau4io1ZvaCi",
        "outputId": "16f7b9e7-0ce0-4e0e-ec01-d6d12a43e471"
      },
      "source": [
        "from keras.models import Model\r\n",
        "from keras.layers import SeparableConv2D, ZeroPadding2D, Activation, Dropout, Dense, \\\r\n",
        "                            Conv2D, MaxPooling2D, Reshape, Permute\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras import Input, optimizers\r\n",
        "\r\n",
        "num_rows = X.shape[1]\r\n",
        "num_columns = X.shape[2]\r\n",
        "num_channels = 1\r\n",
        "\r\n",
        "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\r\n",
        "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\r\n",
        "\r\n",
        "num_labels = y.shape[2]\r\n",
        "#num_labels = 16\r\n",
        "\r\n",
        "def Net(dilated_kernel, dilation, dilated_padding):\r\n",
        "    \r\n",
        "    # input layer\r\n",
        "    inputs = Input(shape=(num_rows, num_columns, num_channels))\r\n",
        "    \r\n",
        "    # DWS-CNN layer 1\r\n",
        "    #x = ZeroPadding2D(padding=(2))(inputs)\r\n",
        "    # use valid padding since padding is introduced before due to its special form (maybe it's equal to use same padding?)\r\n",
        "    x = Conv2D(256,kernel_size=(5,5), strides=(1,1), padding='same')(inputs)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "    # Batch Normalisation before passing it to the next layer\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    # Pooling\r\n",
        "    x = MaxPooling2D(pool_size=(1,5), padding='same')(x)\r\n",
        "    # Dropout\r\n",
        "    x = Dropout(0.25)(x)\r\n",
        "    \r\n",
        "    # DWS-CNN layer 2\r\n",
        "    #x = ZeroPadding2D(padding=(2))(x)\r\n",
        "    # use valid padding since padding is introduced before due to its special form\r\n",
        "    x = Conv2D(256,kernel_size=(5,5), strides=(1,1), padding='same')(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "    # Batch Normalisation before passing it to the next layer\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    # Pooling\r\n",
        "    x = MaxPooling2D(pool_size=(1,4), padding='same')(x)\r\n",
        "    # Dropout\r\n",
        "    x = Dropout(0.25)(x)\r\n",
        "    \r\n",
        "    # DWS-CNN layer 3\r\n",
        "    #x = ZeroPadding2D(padding=(2))(x)\r\n",
        "    # use valid padding since padding is introduced before due to its special form\r\n",
        "    x = Conv2D(256,kernel_size=(5,5), strides=(1,1), padding='same')(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "    # Batch Normalisation before passing it to the next layer\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    # Pooling\r\n",
        "    x = MaxPooling2D(pool_size=(1,2), padding='same')(x)\r\n",
        "    # Dropout\r\n",
        "    x = Dropout(0.25)(x)\r\n",
        "    \r\n",
        "    #x = Reshape((1024, 256))(x)\r\n",
        "    #x = Reshape((1024, 256, 1))(x)\r\n",
        "    x = Permute((1,3,2))(x)\r\n",
        "    #x = Permute((3,2,1))(x)\r\n",
        "    \r\n",
        "    # DIL-CNN \r\n",
        "    x = ZeroPadding2D(padding=(dilated_padding*dilation, 0))(x)\r\n",
        "    x = Conv2D(256, kernel_size=dilated_kernel, dilation_rate=(dilation, 1))(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    initializer = keras.initializers.Ones()\r\n",
        "    x = Conv2D(256, (1,1), strides=(1,3), kernel_initializer=initializer, trainable=False)(x)\r\n",
        "    \r\n",
        "    #x = Permute((2,1,3))(x)\r\n",
        "    \r\n",
        "    x = Reshape((1024, 256*85))(x)\r\n",
        "    \r\n",
        "    # classifier layer\r\n",
        "    outputs = Dense(num_labels,activation='sigmoid')(x)\r\n",
        "    \r\n",
        "    \r\n",
        "    # model compilation for training\r\n",
        "    adam = optimizers.Adam(lr=0.0001)\r\n",
        "    model = Model(inputs, outputs)                            \r\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[\"binary_accuracy\"])\r\n",
        "    \r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "# create the model\r\n",
        "dilated_kernel = (3,3)\r\n",
        "dilation = 10\r\n",
        "dilated_padding = 1\r\n",
        "model = Net(dilated_kernel,dilation,dilated_padding)\r\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1024, 40, 1)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 1024, 40, 256)     6656      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 1024, 40, 256)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1024, 40, 256)     1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 1024, 8, 256)      0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024, 8, 256)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 1024, 8, 256)      1638656   \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1024, 8, 256)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1024, 8, 256)      1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1024, 2, 256)      0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024, 2, 256)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1024, 2, 256)      1638656   \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1024, 2, 256)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1024, 2, 256)      1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1024, 1, 256)      0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024, 1, 256)      0         \n",
            "_________________________________________________________________\n",
            "permute (Permute)            (None, 1024, 256, 1)      0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2 (None, 1044, 256, 1)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 1024, 254, 256)    2560      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1024, 254, 256)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1024, 254, 256)    1024      \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 1024, 85, 256)     65792     \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 1024, 21760)       0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024, 16)          348176    \n",
            "=================================================================\n",
            "Total params: 3,704,592\n",
            "Trainable params: 3,636,752\n",
            "Non-trainable params: 67,840\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-DedsknrpJn"
      },
      "source": [
        "## new model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ICNx-O8vhbQ",
        "outputId": "b312fbc4-6dd8-4bdb-e7be-b750e410dc9a"
      },
      "source": [
        "from keras.models import Model\r\n",
        "from keras.layers import SeparableConv2D, ZeroPadding2D, Activation, Dropout, Dense, \\\r\n",
        "                            Conv2D, MaxPooling2D, Reshape, Permute\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras import Input, optimizers\r\n",
        "\r\n",
        "num_rows = X.shape[1]\r\n",
        "num_columns = X.shape[2]\r\n",
        "num_channels = 1\r\n",
        "\r\n",
        "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\r\n",
        "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\r\n",
        "\r\n",
        "num_labels = y.shape[2]\r\n",
        "#num_labels = 16\r\n",
        "\r\n",
        "def Net(dilated_kernel, dilation, dilated_padding):\r\n",
        "    \r\n",
        "    # input layer\r\n",
        "    inputs = Input(shape=(num_rows, num_columns, num_channels))\r\n",
        "    \r\n",
        "    # DWS-CNN layer 1\r\n",
        "    x = ZeroPadding2D(padding=(2))(inputs)\r\n",
        "    # use valid padding since padding is introduced before due to its special form (maybe it's equal to use same padding?)\r\n",
        "    x = SeparableConv2D(256,kernel_size=(5,5), strides=(1,1), padding='valid')(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "    # Batch Normalisation before passing it to the next layer\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    # Pooling\r\n",
        "    x = MaxPooling2D(pool_size=(1,5), strides=(1,5), padding='valid')(x)\r\n",
        "    # Dropout\r\n",
        "    x = Dropout(0.25)(x)\r\n",
        "    \r\n",
        "    # DWS-CNN layer 2\r\n",
        "    x = ZeroPadding2D(padding=(2))(x)\r\n",
        "    # use valid padding since padding is introduced before due to its special form\r\n",
        "    x = SeparableConv2D(256,kernel_size=(5,5), strides=(1,1), padding='valid')(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "    # Batch Normalisation before passing it to the next layer\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    # Pooling\r\n",
        "    x = MaxPooling2D(pool_size=(1,4), strides=(1,4), padding='valid')(x)\r\n",
        "    # Dropout\r\n",
        "    x = Dropout(0.25)(x)\r\n",
        "    \r\n",
        "    # DWS-CNN layer 3\r\n",
        "    x = ZeroPadding2D(padding=(2))(x)\r\n",
        "    # use valid padding since padding is introduced before due to its special form\r\n",
        "    x = SeparableConv2D(256,kernel_size=(5,5), strides=(1,1), padding='valid')(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "    # Batch Normalisation before passing it to the next layer\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    # Pooling\r\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2), padding='valid')(x)\r\n",
        "    # Dropout\r\n",
        "    x = Dropout(0.25)(x)\r\n",
        "    \r\n",
        "    x = Permute((1,3,2))(x)\r\n",
        "    \r\n",
        "    # DIL-CNN \r\n",
        "    x = ZeroPadding2D(padding=(dilated_padding*dilation, 0))(x)\r\n",
        "    x = Conv2D(256, kernel_size=dilated_kernel, dilation_rate=(dilation, 1))(x)#, strides=(1,3))(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    initializer = keras.initializers.Ones()\r\n",
        "    x = Conv2D(256, (1,1), strides=(1,3), kernel_initializer=initializer)(x)\r\n",
        "    \r\n",
        "    x = Reshape((1024, 256*84))(x)\r\n",
        "    \r\n",
        "    # classifier layer\r\n",
        "    outputs = Dense(num_labels,activation='sigmoid')(x)\r\n",
        "    \r\n",
        "    \r\n",
        "    # model compilation for training\r\n",
        "    adam = optimizers.Adam()#lr=0.0001)\r\n",
        "    model = Model(inputs, outputs)                            \r\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[\"binary_accuracy\"])\r\n",
        "    \r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "# create the model\r\n",
        "dilated_kernel = (5,5)\r\n",
        "dilation = 10\r\n",
        "dilated_padding = 2\r\n",
        "model = Net(dilated_kernel,dilation,dilated_padding)\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 1024, 40, 1)]     0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_11 (ZeroPaddi (None, 1028, 44, 1)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_9 (Separabl (None, 1024, 40, 256)     537       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 1024, 40, 256)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 1024, 40, 256)     1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 1024, 8, 256)      0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 1024, 8, 256)      0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_12 (ZeroPaddi (None, 1028, 12, 256)     0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_10 (Separab (None, 1024, 8, 256)      72192     \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 1024, 8, 256)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 1024, 8, 256)      1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 1024, 2, 256)      0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 1024, 2, 256)      0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_13 (ZeroPaddi (None, 1028, 6, 256)      0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_11 (Separab (None, 1024, 2, 256)      72192     \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 1024, 2, 256)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 1024, 2, 256)      1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 1024, 1, 256)      0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 1024, 1, 256)      0         \n",
            "_________________________________________________________________\n",
            "permute_2 (Permute)          (None, 1024, 256, 1)      0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_14 (ZeroPaddi (None, 1064, 256, 1)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 1024, 252, 256)    6656      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 1024, 252, 256)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 1024, 252, 256)    1024      \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 1024, 84, 256)     65792     \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 1024, 21504)       0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024, 16)          344080    \n",
            "=================================================================\n",
            "Total params: 565,545\n",
            "Trainable params: 563,497\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNzMSCbW4Cv5"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZbIeHH1vheD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd2fdd0-e258-4181-ac47-544f98a2450c"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\r\n",
        "from datetime import datetime \r\n",
        "\r\n",
        "num_epochs = 250\r\n",
        "# low batch size due to memory maximum dimension, modify if using smaller dataset\r\n",
        "num_batch_size = 8\r\n",
        "\r\n",
        "callbacks = [ModelCheckpoint(filepath='/content/drive/MyDrive/model-{val_loss:.2f}_dilated.h5', \r\n",
        "                               verbose=1, save_best_only=True, monitor=\"val_loss\"),\r\n",
        "                EarlyStopping(monitor='val_loss', patience=30)]\r\n",
        "\r\n",
        "start = datetime.now()\r\n",
        "\r\n",
        "#y_train = y_train.reshape(y_train.shape[0], 1024, 16)\r\n",
        "#y_test = y_test.reshape(y_test.shape[0], 1024, 16)\r\n",
        "\r\n",
        "history = model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_split=0.16, verbose=1, callbacks=callbacks)\r\n",
        "\r\n",
        "duration = datetime.now() - start\r\n",
        "print(\"Training completed in time: \", duration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "284/284 [==============================] - 240s 713ms/step - loss: 5.0764 - binary_accuracy: 0.8528 - val_loss: 0.4965 - val_binary_accuracy: 0.7738\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.49648, saving model to /content/drive/MyDrive/model-0.50_dilated.h5\n",
            "Epoch 2/250\n",
            "284/284 [==============================] - 198s 697ms/step - loss: 0.5189 - binary_accuracy: 0.9003 - val_loss: 0.7069 - val_binary_accuracy: 0.6999\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.49648\n",
            "Epoch 3/250\n",
            "284/284 [==============================] - 198s 697ms/step - loss: 0.3778 - binary_accuracy: 0.9100 - val_loss: 1.3049 - val_binary_accuracy: 0.6095\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.49648\n",
            "Epoch 4/250\n",
            "284/284 [==============================] - 197s 695ms/step - loss: 0.2947 - binary_accuracy: 0.9202 - val_loss: 0.8496 - val_binary_accuracy: 0.7642\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.49648\n",
            "Epoch 5/250\n",
            "284/284 [==============================] - 197s 694ms/step - loss: 0.2628 - binary_accuracy: 0.9273 - val_loss: 0.4373 - val_binary_accuracy: 0.8162\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.49648 to 0.43726, saving model to /content/drive/MyDrive/model-0.44_dilated.h5\n",
            "Epoch 6/250\n",
            "284/284 [==============================] - 197s 693ms/step - loss: 0.2258 - binary_accuracy: 0.9341 - val_loss: 0.5767 - val_binary_accuracy: 0.7513\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.43726\n",
            "Epoch 7/250\n",
            "284/284 [==============================] - 197s 693ms/step - loss: 0.2068 - binary_accuracy: 0.9373 - val_loss: 0.3333 - val_binary_accuracy: 0.8830\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.43726 to 0.33328, saving model to /content/drive/MyDrive/model-0.33_dilated.h5\n",
            "Epoch 8/250\n",
            "284/284 [==============================] - 197s 693ms/step - loss: 0.1928 - binary_accuracy: 0.9413 - val_loss: 0.2621 - val_binary_accuracy: 0.9108\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.33328 to 0.26212, saving model to /content/drive/MyDrive/model-0.26_dilated.h5\n",
            "Epoch 9/250\n",
            "284/284 [==============================] - 197s 692ms/step - loss: 0.1849 - binary_accuracy: 0.9433 - val_loss: 0.2373 - val_binary_accuracy: 0.9318\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.26212 to 0.23726, saving model to /content/drive/MyDrive/model-0.24_dilated.h5\n",
            "Epoch 10/250\n",
            "284/284 [==============================] - 197s 692ms/step - loss: 0.1787 - binary_accuracy: 0.9447 - val_loss: 0.2014 - val_binary_accuracy: 0.9473\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.23726 to 0.20135, saving model to /content/drive/MyDrive/model-0.20_dilated.h5\n",
            "Epoch 11/250\n",
            "284/284 [==============================] - 197s 692ms/step - loss: 0.1720 - binary_accuracy: 0.9465 - val_loss: 0.1981 - val_binary_accuracy: 0.9538\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.20135 to 0.19806, saving model to /content/drive/MyDrive/model-0.20_dilated.h5\n",
            "Epoch 12/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1678 - binary_accuracy: 0.9478 - val_loss: 0.1947 - val_binary_accuracy: 0.9535\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.19806 to 0.19471, saving model to /content/drive/MyDrive/model-0.19_dilated.h5\n",
            "Epoch 13/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1639 - binary_accuracy: 0.9489 - val_loss: 0.1919 - val_binary_accuracy: 0.9542\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.19471 to 0.19185, saving model to /content/drive/MyDrive/model-0.19_dilated.h5\n",
            "Epoch 14/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1596 - binary_accuracy: 0.9501 - val_loss: 0.1916 - val_binary_accuracy: 0.9557\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.19185 to 0.19156, saving model to /content/drive/MyDrive/model-0.19_dilated.h5\n",
            "Epoch 15/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1581 - binary_accuracy: 0.9506 - val_loss: 0.1974 - val_binary_accuracy: 0.9566\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.19156\n",
            "Epoch 16/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1560 - binary_accuracy: 0.9516 - val_loss: 0.1873 - val_binary_accuracy: 0.9556\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.19156 to 0.18728, saving model to /content/drive/MyDrive/model-0.19_dilated.h5\n",
            "Epoch 17/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1535 - binary_accuracy: 0.9523 - val_loss: 0.1943 - val_binary_accuracy: 0.9574\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.18728\n",
            "Epoch 18/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1535 - binary_accuracy: 0.9519 - val_loss: 0.2004 - val_binary_accuracy: 0.9558\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.18728\n",
            "Epoch 19/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1529 - binary_accuracy: 0.9521 - val_loss: 0.3468 - val_binary_accuracy: 0.8607\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.18728\n",
            "Epoch 20/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1473 - binary_accuracy: 0.9542 - val_loss: 0.2357 - val_binary_accuracy: 0.9379\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.18728\n",
            "Epoch 21/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1479 - binary_accuracy: 0.9537 - val_loss: 0.2080 - val_binary_accuracy: 0.9557\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.18728\n",
            "Epoch 22/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1489 - binary_accuracy: 0.9533 - val_loss: 0.1844 - val_binary_accuracy: 0.9563\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.18728 to 0.18444, saving model to /content/drive/MyDrive/model-0.18_dilated.h5\n",
            "Epoch 23/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1473 - binary_accuracy: 0.9539 - val_loss: 0.2193 - val_binary_accuracy: 0.9441\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.18444\n",
            "Epoch 24/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1436 - binary_accuracy: 0.9557 - val_loss: 0.2035 - val_binary_accuracy: 0.9326\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.18444\n",
            "Epoch 25/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1412 - binary_accuracy: 0.9560 - val_loss: 0.2872 - val_binary_accuracy: 0.8922\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.18444\n",
            "Epoch 26/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1408 - binary_accuracy: 0.9566 - val_loss: 0.2524 - val_binary_accuracy: 0.9156\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.18444\n",
            "Epoch 27/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1395 - binary_accuracy: 0.9564 - val_loss: 0.2186 - val_binary_accuracy: 0.9146\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.18444\n",
            "Epoch 28/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1401 - binary_accuracy: 0.9567 - val_loss: 0.2999 - val_binary_accuracy: 0.8846\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.18444\n",
            "Epoch 29/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1353 - binary_accuracy: 0.9584 - val_loss: 0.2213 - val_binary_accuracy: 0.9480\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.18444\n",
            "Epoch 30/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1407 - binary_accuracy: 0.9558 - val_loss: 0.3681 - val_binary_accuracy: 0.8272\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.18444\n",
            "Epoch 31/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1377 - binary_accuracy: 0.9572 - val_loss: 0.2427 - val_binary_accuracy: 0.9032\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.18444\n",
            "Epoch 32/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1317 - binary_accuracy: 0.9595 - val_loss: 0.2157 - val_binary_accuracy: 0.9398\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.18444\n",
            "Epoch 33/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1341 - binary_accuracy: 0.9588 - val_loss: 0.2460 - val_binary_accuracy: 0.9168\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.18444\n",
            "Epoch 34/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1344 - binary_accuracy: 0.9586 - val_loss: 0.2653 - val_binary_accuracy: 0.8843\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.18444\n",
            "Epoch 35/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1316 - binary_accuracy: 0.9594 - val_loss: 0.4813 - val_binary_accuracy: 0.7918\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.18444\n",
            "Epoch 36/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1332 - binary_accuracy: 0.9587 - val_loss: 0.1705 - val_binary_accuracy: 0.9527\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.18444 to 0.17048, saving model to /content/drive/MyDrive/model-0.17_dilated.h5\n",
            "Epoch 37/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1309 - binary_accuracy: 0.9597 - val_loss: 0.2137 - val_binary_accuracy: 0.9389\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.17048\n",
            "Epoch 38/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1315 - binary_accuracy: 0.9596 - val_loss: 0.7942 - val_binary_accuracy: 0.6780\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.17048\n",
            "Epoch 39/250\n",
            "284/284 [==============================] - 196s 690ms/step - loss: 0.1303 - binary_accuracy: 0.9598 - val_loss: 0.4231 - val_binary_accuracy: 0.8422\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.17048\n",
            "Epoch 40/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1295 - binary_accuracy: 0.9600 - val_loss: 0.5233 - val_binary_accuracy: 0.8341\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.17048\n",
            "Epoch 41/250\n",
            "284/284 [==============================] - 196s 690ms/step - loss: 0.1244 - binary_accuracy: 0.9619 - val_loss: 0.5150 - val_binary_accuracy: 0.7839\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.17048\n",
            "Epoch 42/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1241 - binary_accuracy: 0.9614 - val_loss: 0.2203 - val_binary_accuracy: 0.9131\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.17048\n",
            "Epoch 43/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1268 - binary_accuracy: 0.9610 - val_loss: 0.1979 - val_binary_accuracy: 0.9478\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.17048\n",
            "Epoch 44/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1248 - binary_accuracy: 0.9617 - val_loss: 0.2481 - val_binary_accuracy: 0.9204\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.17048\n",
            "Epoch 45/250\n",
            "284/284 [==============================] - 196s 690ms/step - loss: 0.1246 - binary_accuracy: 0.9619 - val_loss: 0.3173 - val_binary_accuracy: 0.8839\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.17048\n",
            "Epoch 46/250\n",
            "284/284 [==============================] - 196s 690ms/step - loss: 0.1224 - binary_accuracy: 0.9626 - val_loss: 0.9708 - val_binary_accuracy: 0.7705\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.17048\n",
            "Epoch 47/250\n",
            "284/284 [==============================] - 196s 690ms/step - loss: 0.1272 - binary_accuracy: 0.9608 - val_loss: 0.4201 - val_binary_accuracy: 0.8334\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.17048\n",
            "Epoch 48/250\n",
            "284/284 [==============================] - 196s 690ms/step - loss: 0.1263 - binary_accuracy: 0.9609 - val_loss: 0.4418 - val_binary_accuracy: 0.8158\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.17048\n",
            "Epoch 49/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1227 - binary_accuracy: 0.9622 - val_loss: 0.5643 - val_binary_accuracy: 0.7541\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.17048\n",
            "Epoch 50/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1222 - binary_accuracy: 0.9625 - val_loss: 0.4790 - val_binary_accuracy: 0.7968\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.17048\n",
            "Epoch 51/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1211 - binary_accuracy: 0.9630 - val_loss: 0.2614 - val_binary_accuracy: 0.8935\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.17048\n",
            "Epoch 52/250\n",
            "284/284 [==============================] - 196s 690ms/step - loss: 0.1203 - binary_accuracy: 0.9631 - val_loss: 0.3280 - val_binary_accuracy: 0.8867\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.17048\n",
            "Epoch 53/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1223 - binary_accuracy: 0.9623 - val_loss: 0.7187 - val_binary_accuracy: 0.7246\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.17048\n",
            "Epoch 54/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1196 - binary_accuracy: 0.9632 - val_loss: 0.4940 - val_binary_accuracy: 0.7590\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.17048\n",
            "Epoch 55/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1191 - binary_accuracy: 0.9632 - val_loss: 0.2385 - val_binary_accuracy: 0.9224\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.17048\n",
            "Epoch 56/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1189 - binary_accuracy: 0.9631 - val_loss: 0.4595 - val_binary_accuracy: 0.8163\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.17048\n",
            "Epoch 57/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1185 - binary_accuracy: 0.9635 - val_loss: 0.2246 - val_binary_accuracy: 0.9194\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.17048\n",
            "Epoch 58/250\n",
            "284/284 [==============================] - 196s 690ms/step - loss: 0.1181 - binary_accuracy: 0.9636 - val_loss: 0.5259 - val_binary_accuracy: 0.7873\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.17048\n",
            "Epoch 59/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1150 - binary_accuracy: 0.9647 - val_loss: 0.3284 - val_binary_accuracy: 0.8778\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.17048\n",
            "Epoch 60/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1165 - binary_accuracy: 0.9643 - val_loss: 0.2485 - val_binary_accuracy: 0.9075\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.17048\n",
            "Epoch 61/250\n",
            "284/284 [==============================] - 196s 690ms/step - loss: 0.1192 - binary_accuracy: 0.9635 - val_loss: 0.1886 - val_binary_accuracy: 0.9427\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.17048\n",
            "Epoch 62/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1153 - binary_accuracy: 0.9646 - val_loss: 0.1503 - val_binary_accuracy: 0.9609\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.17048 to 0.15027, saving model to /content/drive/MyDrive/model-0.15_dilated.h5\n",
            "Epoch 63/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1144 - binary_accuracy: 0.9649 - val_loss: 0.2517 - val_binary_accuracy: 0.9092\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.15027\n",
            "Epoch 64/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1155 - binary_accuracy: 0.9646 - val_loss: 0.2216 - val_binary_accuracy: 0.9180\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.15027\n",
            "Epoch 65/250\n",
            "284/284 [==============================] - 196s 690ms/step - loss: 0.1144 - binary_accuracy: 0.9649 - val_loss: 0.1650 - val_binary_accuracy: 0.9566\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.15027\n",
            "Epoch 66/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1160 - binary_accuracy: 0.9643 - val_loss: 0.3326 - val_binary_accuracy: 0.8476\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.15027\n",
            "Epoch 67/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1136 - binary_accuracy: 0.9652 - val_loss: 0.4126 - val_binary_accuracy: 0.8093\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.15027\n",
            "Epoch 68/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1172 - binary_accuracy: 0.9641 - val_loss: 0.2635 - val_binary_accuracy: 0.8844\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.15027\n",
            "Epoch 69/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1136 - binary_accuracy: 0.9651 - val_loss: 0.2307 - val_binary_accuracy: 0.8956\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.15027\n",
            "Epoch 70/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1138 - binary_accuracy: 0.9652 - val_loss: 0.1399 - val_binary_accuracy: 0.9633\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.15027 to 0.13995, saving model to /content/drive/MyDrive/model-0.14_dilated.h5\n",
            "Epoch 71/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1132 - binary_accuracy: 0.9651 - val_loss: 0.2254 - val_binary_accuracy: 0.9159\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.13995\n",
            "Epoch 72/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1118 - binary_accuracy: 0.9659 - val_loss: 0.5456 - val_binary_accuracy: 0.8259\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.13995\n",
            "Epoch 73/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1141 - binary_accuracy: 0.9650 - val_loss: 0.2985 - val_binary_accuracy: 0.8748\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.13995\n",
            "Epoch 74/250\n",
            "284/284 [==============================] - 196s 690ms/step - loss: 0.1131 - binary_accuracy: 0.9651 - val_loss: 0.3073 - val_binary_accuracy: 0.8679\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.13995\n",
            "Epoch 75/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1128 - binary_accuracy: 0.9654 - val_loss: 0.1430 - val_binary_accuracy: 0.9632\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.13995\n",
            "Epoch 76/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1136 - binary_accuracy: 0.9655 - val_loss: 0.1462 - val_binary_accuracy: 0.9608\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.13995\n",
            "Epoch 77/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1113 - binary_accuracy: 0.9658 - val_loss: 0.1371 - val_binary_accuracy: 0.9612\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.13995 to 0.13712, saving model to /content/drive/MyDrive/model-0.14_dilated.h5\n",
            "Epoch 78/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1085 - binary_accuracy: 0.9667 - val_loss: 0.1629 - val_binary_accuracy: 0.9556\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.13712\n",
            "Epoch 79/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1108 - binary_accuracy: 0.9659 - val_loss: 0.1479 - val_binary_accuracy: 0.9578\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.13712\n",
            "Epoch 80/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1090 - binary_accuracy: 0.9667 - val_loss: 0.1832 - val_binary_accuracy: 0.9510\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.13712\n",
            "Epoch 81/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1081 - binary_accuracy: 0.9670 - val_loss: 0.3166 - val_binary_accuracy: 0.8783\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.13712\n",
            "Epoch 82/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1080 - binary_accuracy: 0.9671 - val_loss: 0.1370 - val_binary_accuracy: 0.9616\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.13712 to 0.13695, saving model to /content/drive/MyDrive/model-0.14_dilated.h5\n",
            "Epoch 83/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1097 - binary_accuracy: 0.9664 - val_loss: 0.1733 - val_binary_accuracy: 0.9464\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.13695\n",
            "Epoch 84/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1079 - binary_accuracy: 0.9671 - val_loss: 0.1290 - val_binary_accuracy: 0.9642\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.13695 to 0.12898, saving model to /content/drive/MyDrive/model-0.13_dilated.h5\n",
            "Epoch 85/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1085 - binary_accuracy: 0.9667 - val_loss: 0.2917 - val_binary_accuracy: 0.8757\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.12898\n",
            "Epoch 86/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1069 - binary_accuracy: 0.9672 - val_loss: 0.1883 - val_binary_accuracy: 0.9454\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.12898\n",
            "Epoch 87/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1079 - binary_accuracy: 0.9671 - val_loss: 0.1222 - val_binary_accuracy: 0.9647\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.12898 to 0.12223, saving model to /content/drive/MyDrive/model-0.12_dilated.h5\n",
            "Epoch 88/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1075 - binary_accuracy: 0.9668 - val_loss: 0.1140 - val_binary_accuracy: 0.9665\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.12223 to 0.11399, saving model to /content/drive/MyDrive/model-0.11_dilated.h5\n",
            "Epoch 89/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1047 - binary_accuracy: 0.9680 - val_loss: 0.2333 - val_binary_accuracy: 0.9119\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.11399\n",
            "Epoch 90/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1062 - binary_accuracy: 0.9675 - val_loss: 0.1643 - val_binary_accuracy: 0.9518\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.11399\n",
            "Epoch 91/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1042 - binary_accuracy: 0.9682 - val_loss: 0.1559 - val_binary_accuracy: 0.9590\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.11399\n",
            "Epoch 92/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1046 - binary_accuracy: 0.9680 - val_loss: 0.1337 - val_binary_accuracy: 0.9629\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.11399\n",
            "Epoch 93/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1079 - binary_accuracy: 0.9666 - val_loss: 0.1249 - val_binary_accuracy: 0.9645\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.11399\n",
            "Epoch 94/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1055 - binary_accuracy: 0.9671 - val_loss: 0.1260 - val_binary_accuracy: 0.9637\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.11399\n",
            "Epoch 95/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1050 - binary_accuracy: 0.9678 - val_loss: 0.1271 - val_binary_accuracy: 0.9649\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.11399\n",
            "Epoch 96/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1019 - binary_accuracy: 0.9689 - val_loss: 0.1261 - val_binary_accuracy: 0.9647\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.11399\n",
            "Epoch 97/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1034 - binary_accuracy: 0.9683 - val_loss: 0.1326 - val_binary_accuracy: 0.9645\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.11399\n",
            "Epoch 98/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1034 - binary_accuracy: 0.9682 - val_loss: 0.1403 - val_binary_accuracy: 0.9644\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.11399\n",
            "Epoch 99/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1033 - binary_accuracy: 0.9681 - val_loss: 0.1426 - val_binary_accuracy: 0.9645\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.11399\n",
            "Epoch 100/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1020 - binary_accuracy: 0.9689 - val_loss: 0.1248 - val_binary_accuracy: 0.9643\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.11399\n",
            "Epoch 101/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1044 - binary_accuracy: 0.9678 - val_loss: 0.1250 - val_binary_accuracy: 0.9656\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.11399\n",
            "Epoch 102/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1016 - binary_accuracy: 0.9690 - val_loss: 0.1410 - val_binary_accuracy: 0.9611\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.11399\n",
            "Epoch 103/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1039 - binary_accuracy: 0.9681 - val_loss: 0.1801 - val_binary_accuracy: 0.9389\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.11399\n",
            "Epoch 104/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1041 - binary_accuracy: 0.9680 - val_loss: 0.1237 - val_binary_accuracy: 0.9652\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.11399\n",
            "Epoch 105/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1008 - binary_accuracy: 0.9692 - val_loss: 0.1281 - val_binary_accuracy: 0.9643\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.11399\n",
            "Epoch 106/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1011 - binary_accuracy: 0.9693 - val_loss: 0.1265 - val_binary_accuracy: 0.9650\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.11399\n",
            "Epoch 107/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1020 - binary_accuracy: 0.9686 - val_loss: 0.1125 - val_binary_accuracy: 0.9669\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.11399 to 0.11248, saving model to /content/drive/MyDrive/model-0.11_dilated.h5\n",
            "Epoch 108/250\n",
            "284/284 [==============================] - 197s 693ms/step - loss: 0.1011 - binary_accuracy: 0.9690 - val_loss: 0.1236 - val_binary_accuracy: 0.9644\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.11248\n",
            "Epoch 109/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1024 - binary_accuracy: 0.9689 - val_loss: 0.1774 - val_binary_accuracy: 0.9339\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.11248\n",
            "Epoch 110/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.0995 - binary_accuracy: 0.9695 - val_loss: 0.1245 - val_binary_accuracy: 0.9647\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.11248\n",
            "Epoch 111/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1020 - binary_accuracy: 0.9685 - val_loss: 0.1148 - val_binary_accuracy: 0.9666\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.11248\n",
            "Epoch 112/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.0996 - binary_accuracy: 0.9695 - val_loss: 0.1331 - val_binary_accuracy: 0.9640\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.11248\n",
            "Epoch 113/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1024 - binary_accuracy: 0.9686 - val_loss: 0.1180 - val_binary_accuracy: 0.9661\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.11248\n",
            "Epoch 114/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.0999 - binary_accuracy: 0.9693 - val_loss: 0.1226 - val_binary_accuracy: 0.9654\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.11248\n",
            "Epoch 115/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.0999 - binary_accuracy: 0.9692 - val_loss: 0.1252 - val_binary_accuracy: 0.9666\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.11248\n",
            "Epoch 116/250\n",
            "284/284 [==============================] - 197s 693ms/step - loss: 0.1000 - binary_accuracy: 0.9691 - val_loss: 0.1202 - val_binary_accuracy: 0.9659\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.11248\n",
            "Epoch 117/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.1002 - binary_accuracy: 0.9689 - val_loss: 0.1175 - val_binary_accuracy: 0.9662\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.11248\n",
            "Epoch 118/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.0973 - binary_accuracy: 0.9697 - val_loss: 0.1170 - val_binary_accuracy: 0.9667\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.11248\n",
            "Epoch 119/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.0990 - binary_accuracy: 0.9698 - val_loss: 0.1212 - val_binary_accuracy: 0.9669\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.11248\n",
            "Epoch 120/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.1001 - binary_accuracy: 0.9693 - val_loss: 0.1210 - val_binary_accuracy: 0.9656\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.11248\n",
            "Epoch 121/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.0957 - binary_accuracy: 0.9705 - val_loss: 0.1433 - val_binary_accuracy: 0.9612\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.11248\n",
            "Epoch 122/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.0958 - binary_accuracy: 0.9707 - val_loss: 0.1389 - val_binary_accuracy: 0.9638\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.11248\n",
            "Epoch 123/250\n",
            "284/284 [==============================] - 197s 692ms/step - loss: 0.0985 - binary_accuracy: 0.9696 - val_loss: 0.1213 - val_binary_accuracy: 0.9657\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.11248\n",
            "Epoch 124/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.0972 - binary_accuracy: 0.9703 - val_loss: 0.1270 - val_binary_accuracy: 0.9637\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.11248\n",
            "Epoch 125/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.0983 - binary_accuracy: 0.9697 - val_loss: 0.1164 - val_binary_accuracy: 0.9662\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.11248\n",
            "Epoch 126/250\n",
            "284/284 [==============================] - 197s 692ms/step - loss: 0.0976 - binary_accuracy: 0.9699 - val_loss: 0.1204 - val_binary_accuracy: 0.9657\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.11248\n",
            "Epoch 127/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.0964 - binary_accuracy: 0.9702 - val_loss: 0.2113 - val_binary_accuracy: 0.9218\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.11248\n",
            "Epoch 128/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.0971 - binary_accuracy: 0.9701 - val_loss: 0.1164 - val_binary_accuracy: 0.9660\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.11248\n",
            "Epoch 129/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.0949 - binary_accuracy: 0.9708 - val_loss: 0.1159 - val_binary_accuracy: 0.9670\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.11248\n",
            "Epoch 130/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.0952 - binary_accuracy: 0.9706 - val_loss: 0.2048 - val_binary_accuracy: 0.9374\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.11248\n",
            "Epoch 131/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.0935 - binary_accuracy: 0.9713 - val_loss: 0.1159 - val_binary_accuracy: 0.9661\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.11248\n",
            "Epoch 132/250\n",
            "284/284 [==============================] - 197s 692ms/step - loss: 0.0980 - binary_accuracy: 0.9699 - val_loss: 0.1152 - val_binary_accuracy: 0.9669\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.11248\n",
            "Epoch 133/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.0957 - binary_accuracy: 0.9706 - val_loss: 0.1260 - val_binary_accuracy: 0.9647\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.11248\n",
            "Epoch 134/250\n",
            "284/284 [==============================] - 196s 691ms/step - loss: 0.0973 - binary_accuracy: 0.9701 - val_loss: 0.1167 - val_binary_accuracy: 0.9664\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.11248\n",
            "Epoch 135/250\n",
            "284/284 [==============================] - 196s 692ms/step - loss: 0.0963 - binary_accuracy: 0.9705 - val_loss: 0.1236 - val_binary_accuracy: 0.9661\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.11248\n",
            "Epoch 136/250\n",
            "174/284 [=================>............] - ETA: 1:10 - loss: 0.0922 - binary_accuracy: 0.9717"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D80dzXbzmjpB"
      },
      "source": [
        "model.save('/content/drive/MyDrive/baseline-dilated-0.11.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcgsF2fZvhgj"
      },
      "source": [
        "# Evaluate the model on the test data using `evaluate`\r\n",
        "print(\"Evaluate on test data\")\r\n",
        "results = model.evaluate(x_test, y_test, batch_size=num_batch_size)\r\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "z2ZOVgAMvhil",
        "outputId": "43193974-176f-4540-e3fb-21c175f96ba0"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# summarize history for accuracy\r\n",
        "plt.plot(history.history['binary_accuracy'])\r\n",
        "plt.plot(history.history['val_binary_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "# summarize history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e5485f10d997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'binary_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_binary_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZkGJtk6pRcu"
      },
      "source": [
        "model2 = keras.models.load_model('/content/drive/MyDrive/model-0.11_dilated.h5')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jAOtu-FvhlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb41557-847b-4d85-e502-b74ed01def4a"
      },
      "source": [
        "TP = 0\r\n",
        "FP = 0\r\n",
        "TN = 0\r\n",
        "FN = 0\r\n",
        "\r\n",
        "y_pred = model2.predict(x_test, batch_size=8, verbose=1)\r\n",
        "\r\n",
        "for i in range(y_test.shape[0]):\r\n",
        "    if i%100 == 0:\r\n",
        "        print(\"processing element \", i)\r\n",
        "    for j in range(y_test.shape[1]):\r\n",
        "        for k in range(y_test.shape[2]):\r\n",
        "            \r\n",
        "            test = y_test[i][j][k]\r\n",
        "            pred = y_pred[i][j][k]\r\n",
        "            \r\n",
        "            # binarization of predicted output\r\n",
        "            if(pred >= 0.5):\r\n",
        "                pred = 1\r\n",
        "            else:\r\n",
        "                pred = 0\r\n",
        "\r\n",
        "            if(test == 1 and pred == 1):\r\n",
        "                TP = TP + 1\r\n",
        "            elif(test == 0 and pred == 1):\r\n",
        "                FP = FP + 1\r\n",
        "            elif(test == 0 and pred == 0):\r\n",
        "                TN = TN + 1\r\n",
        "            elif(test == 1 and pred == 0):\r\n",
        "                FN = FN + 1\r\n",
        "                \r\n",
        "#print(TP+FP+TN+FN)\r\n",
        "#print(y_test.shape[1]*y_test.shape[2]*y_test.shape[0])\r\n",
        "\r\n",
        "recall = TP/(TP+FN)\r\n",
        "\r\n",
        "precision = TP/(TP+FP)\r\n",
        "\r\n",
        "f1_score = 2*recall*precision/(recall+precision)\r\n",
        "\r\n",
        "print(\"recall: \", recall)\r\n",
        "print(\"precision: \", precision)\r\n",
        "print(\"f1: \", f1_score)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "85/85 [==============================] - 57s 267ms/step\n",
            "processing element  0\n",
            "processing element  100\n",
            "processing element  200\n",
            "processing element  300\n",
            "processing element  400\n",
            "processing element  500\n",
            "processing element  600\n",
            "recall:  0.649669367374582\n",
            "precision:  0.8328207493977139\n",
            "f1:  0.7299315162729577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvaDrIHpvhnG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b184ff-c819-4528-d7c5-382dfa7b114a"
      },
      "source": [
        "TP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "528734"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTAioOcvhpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7d03ce-3764-488a-a56e-11dd182813c9"
      },
      "source": [
        "print(\"f1: \", f1_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1:  0.5969019823910721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0V-hVJgdEmg"
      },
      "source": [
        "## save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ebi1aSfc7zr"
      },
      "source": [
        "filename = '/content/drive/MyDrive/model-dessed-endtrain.h5'\r\n",
        "model.save(filename)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}